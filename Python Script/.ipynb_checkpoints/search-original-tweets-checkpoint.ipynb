{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb42952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887abcc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dce7fe3c",
   "metadata": {},
   "source": [
    "### Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c4b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(df, col_name):\n",
    "    df = df[df[col_name].notna()]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19efec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_manipulation(data): # input data is list of strings (urls)\n",
    "    for i in range (0, len(data)):\n",
    "        if type(data[i]) == str:\n",
    "            data[i]=re.sub('((http|https):\\/\\/www\\.)|((http|https):\\/\\/)', '', data[i]) # remove beginning 'http[s]://www.'\n",
    "            data[i]=re.sub('((\\/\\?(.*)|\\?(.*)))', '', data[i]) # remove ending '?' and everything that comes after\n",
    "            data[i]=data[i].strip('/') #remove ending '/'\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe640bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_manipulation(data): # input is list of strings (headlines/Twitter text)\n",
    "    for i in range (0, len(data)):\n",
    "        if type(data[i]) == str:\n",
    "            data[i] = data[i].lower() # text to lowercase\n",
    "            data[i] = re.sub('(\\\"|\\,|\\'|\\´|\\?|\\\\|\\-|\\—|\\||:|_)', ' ', data[i]) # replace \",'´?\\-—|:_\n",
    "            #data[i] = re.sub('(https|http)(.*)', '', data[i]) # remove http[s]:// from end of text\n",
    "            data[i] = re.sub('redstate', '', data[i]) # remove 'redstate' at end of line\n",
    "            data[i] = re.sub(r\" +\", ' ', data[i]) # remove multiple whitespaces\n",
    "            data[i] = data[i].strip() # strip leading and trailing whitespaces\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e5235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_info(tweets_df):\n",
    "    tweets_df = remove_nan(tweets_df, 'entities') # remove rows that have missing value for 'entities'\n",
    "    indices = tweets_df.index.tolist() # save index to a list (need for join later)\n",
    "    data = {} # empty dict \n",
    "    \n",
    "    # iterate through all rows of column 'entities'; convert string to dict with its corresponding row index as key\n",
    "    for i in range (0, len(tweets_df)):\n",
    "        d = ast.literal_eval(tweets_df.iloc[i, tweets_df.columns.get_loc('entities')])\n",
    "        data[indices[i]] = d\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(data, orient='index') # convert from dictionary to df\n",
    "    \n",
    "    if 'urls' not in df:\n",
    "        tweets_df['expanded_urls_manipulated'] = 'NaN'\n",
    "        tweets_df['unwound_urls_manipulated'] = 'NaN'\n",
    "        return tweets_df\n",
    "    \n",
    "    else: \n",
    "        df = remove_nan(df, 'urls') # remove rows that have missing values for 'urls'\n",
    "        \n",
    "        # iterate through column 'urls', extract relevant infos and combin into df:\n",
    "        expanded_urls = []\n",
    "        unwound_urls = []\n",
    "    \n",
    "        for i in range (0, len(df)):\n",
    "            if 'expanded_url' in df.iloc[i, df.columns.get_loc('urls')][0]:\n",
    "                expanded_urls.append(df.iloc[i, df.columns.get_loc('urls')][0]['expanded_url'])\n",
    "            else:\n",
    "                expanded_urls.append('NaN')\n",
    "        \n",
    "            if 'unwound_url' in df.iloc[i, df.columns.get_loc('urls')][0]:\n",
    "                unwound_urls.append(df.iloc[i, df.columns.get_loc('urls')][0]['unwound_url'])\n",
    "            else:\n",
    "                unwound_urls.append('NaN')\n",
    "        \n",
    "        meta_info = pd.DataFrame(list(zip(expanded_urls, unwound_urls)), #title, description)),\n",
    "                                 columns =['expanded_urls', 'unwound_urls'], #'title_tweet', 'description'],\n",
    "                                 index = df.index.tolist())\n",
    "    \n",
    "        # string manipulation of expanded_urls and unwound_urls: \n",
    "        expanded_urls_manipulated = url_manipulation(expanded_urls)\n",
    "        unwound_urls_manipulated = url_manipulation(unwound_urls)\n",
    "    \n",
    "        # insert manipulated url-columns into meta_info df:\n",
    "        meta_info.insert(1, 'expanded_urls_manipulated', expanded_urls_manipulated)\n",
    "        meta_info.insert(3, 'unwound_urls_manipulated', unwound_urls_manipulated) \n",
    "    \n",
    "        # join meta_info with tweet_df\n",
    "        tweets_expanded = meta_info.join(tweets_df) # join with tweets df\n",
    "    \n",
    "        # cast conversation_id, author_id and id as string values\n",
    "        tweets_expanded['conversation_id'] = tweets_expanded['conversation_id'].astype('str')\n",
    "        tweets_expanded['author_id'] = tweets_expanded['author_id'].astype('str')\n",
    "        tweets_expanded['id'] = tweets_expanded['id'].astype('str')\n",
    "    \n",
    "        return tweets_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2682292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(articles, tweets):\n",
    "    # based on 'article_urls' = 'expanded_url':\n",
    "    matches_1 = articles.merge(tweets, how = 'inner', left_on = 'article_urls_manipulated', right_on = 'expanded_urls_manipulated')\n",
    "    \n",
    "    # based on 'article_urls' = 'unwound_url'\n",
    "    matches_2 = articles.merge(tweets, how = 'inner', left_on = 'article_urls_manipulated', right_on = 'unwound_urls_manipulated')\n",
    "    \n",
    "    # based on 'title' = 'text' \n",
    "    matches_3 = pd.DataFrame(columns=[]) # create empty df\n",
    "    \n",
    "    for i in range (0, len(tweets)):\n",
    "        for j in range (0, len(articles)):\n",
    "            if articles.iloc[j, articles.columns.get_loc('title_manipulated')] in tweets.iloc[i, tweets.columns.get_loc('text_manipulated')]:\n",
    "                matches_3= matches_3.append(pd.concat([tweets.iloc[i, :], articles.iloc[j, :]]).to_frame().transpose())\n",
    "    \n",
    "    \n",
    "    matches = pd.concat([matches_1, matches_2, matches_3]) # concatenate all matched df's\n",
    "    matches.drop_duplicates(subset = ['title', 'id'], keep = 'first', inplace = True) # remove duplicates\n",
    "    \n",
    "    # remaining (unmatched) articles:\n",
    "    remaining_articles = articles[~articles['title'].isin(matches['title'].tolist())]\n",
    "    \n",
    "    matches = pd.concat([matches, remaining_articles]).reset_index(drop=True) # add unmatched articles to matches table\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1b70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a25dcd64",
   "metadata": {},
   "source": [
    "### Loop through List of Outlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba897b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data: \n",
    "article_dates = pd.read_csv('../data/ad_fontes/articles_dates.csv', index_col=0)\n",
    "adfontes_urls = pd.read_csv('../data/ad_fontes/adfontes_urls.csv', encoding='cp1252', index_col=0)\n",
    "tweet_counts = pd.read_csv('../data/twitter/tweet_counts.csv')\n",
    "outlets = pd.read_excel('../data/twitter/twitter_handles.xlsx')\n",
    "\n",
    "\n",
    "# join article_dates with adfontes_urls on 'adfontes_url'\n",
    "joined_data = article_dates.merge(adfontes_urls, how='inner', on='adfontes_url')\n",
    "\n",
    "# join with twitter_handles \n",
    "joined_data = pd.merge(joined_data, outlets, how='inner', left_on='outlet', right_on='sources').drop('sources', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "952ab4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify list of Twitter handles \n",
    "remove = ['comicsandsdaily', 'EveningTimesCC', 'NewYorkSun', 'BoingBoing'] # outlets for which no tweets were scraped\n",
    " \n",
    "twitter_handles = tweet_counts[~tweet_counts['username'].isin(remove)]['username'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4a37af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtype={'text': str, 'author_id': str, 'conversation_id': str, 'id': str, 'entities': str, 'attachments': str, 'referenced_tweets': str, 'withheld': str}\n",
    "parse_dates=['created_at']\n",
    "\n",
    "for handle in twitter_handles:\n",
    "    # read data:\n",
    "    tweets = pd.read_csv(f'data/tweet_collection/{handle}.csv', dtype=dtype, parse_dates=parse_dates)\n",
    "    articles = joined_data[joined_data['twitter_handle']==f'{handle}']\n",
    "    \n",
    "    # get meta info for tweets (= expanded_url & unwound_urls)\n",
    "    tweets_expanded = get_meta_info(tweets)\n",
    "    tweets_expanded.insert(tweets_expanded.columns.get_loc('text')+1, 'text_manipulated', text_manipulation(tweets_expanded['text'].tolist())) # manipulate text and add to df\n",
    "    \n",
    "    # apply string manipulation to article_url and title\n",
    "    articles.insert(articles.columns.get_loc('article_url')+1, 'article_urls_manipulated', url_manipulation(articles['article_url'].tolist()))\n",
    "    articles.insert(articles.columns.get_loc('title')+1, 'title_manipulated', text_manipulation(articles['title'].tolist()))\n",
    "    \n",
    "    # find matches\n",
    "    matches = find_matches(articles, tweets_expanded) # finde matches\n",
    "\n",
    "   # save as csv:\n",
    "    matches.to_csv(f'../data/twitter/article_tweets/{handle}.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165faf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ffb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb407c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831306bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f61eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
