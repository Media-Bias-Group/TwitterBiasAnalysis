{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Note: <br>\nThis code for the fine-tuning process of XLNet for hate speech detection is based on the following example, published on Medium: <br>\n\nlink to article: https://medium.com/swlh/using-xlnet-for-sentiment-classification-cfa948e65e85 <br>\nauthor: Shanay Ghag <br>\npublished at: Jun 16, 2020<br>\nlink to GitHub: https://github.com/shanayghag/Sentiment-classification-using-XLNet","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset_builder, load_dataset\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport re\n\nimport sentencepiece\nfrom collections import defaultdict\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import TensorDataset,RandomSampler,SequentialSampler, Dataset, DataLoader,random_split,SubsetRandomSampler\nimport torch.nn.functional as F\nfrom keras.preprocessing.sequence import pad_sequences\nimport transformers\nfrom transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2022-08-10T15:45:50.207136Z","iopub.execute_input":"2022-08-10T15:45:50.207629Z","iopub.status.idle":"2022-08-10T15:45:50.217428Z","shell.execute_reply.started":"2022-08-10T15:45:50.207593Z","shell.execute_reply":"2022-08-10T15:45:50.216186Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# set up connection to drive: \n#from google.colab import drive\n#drive.mount('/content/drive')\n\n# define device: \ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"executionInfo":{"elapsed":17542,"status":"ok","timestamp":1639927280017,"user":{"displayName":"lisa richter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08999729000846221512"},"user_tz":-60},"id":"g4Qe-YCBVUUa","outputId":"40bb91f5-9d42-40af-bc87-d9bca9c0687d","execution":{"iopub.status.busy":"2022-08-10T15:45:53.396248Z","iopub.execute_input":"2022-08-10T15:45:53.397614Z","iopub.status.idle":"2022-08-10T15:45:53.409801Z","shell.execute_reply.started":"2022-08-10T15:45:53.397553Z","shell.execute_reply":"2022-08-10T15:45:53.408433Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"code","source":"def prepare_text(text):\n    text = re.sub(r\"@[A-Za-z0-9_]+\", ' ', text) # remove @user \n    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text) # remove links\n    text = re.sub(r\"[^a-zA-z.!?'0-9]\", ' ', text) # remove smileys\n    text = re.sub('#', '', text) # remove hash sign\n    text = re.sub('\\t', ' ',  text) # remove tab\n    text = re.sub(r\" +\", ' ', text) # remove multiple whitespaces\n    return text","metadata":{"executionInfo":{"elapsed":205,"status":"ok","timestamp":1639927284695,"user":{"displayName":"lisa richter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08999729000846221512"},"user_tz":-60},"id":"Zc586jVyHNI9","execution":{"iopub.status.busy":"2022-08-10T15:45:55.309362Z","iopub.execute_input":"2022-08-10T15:45:55.309845Z","iopub.status.idle":"2022-08-10T15:45:55.317242Z","shell.execute_reply.started":"2022-08-10T15:45:55.309791Z","shell.execute_reply":"2022-08-10T15:45:55.315954Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# 1. Prepare Hate Dataset for Fine-Tuning","metadata":{"id":"CnPDjd1bsiBQ"}},{"cell_type":"code","source":"# load dataset from hugging face hub: tweets_hate_speech_detection\nhate_data_train = load_dataset('tweets_hate_speech_detection', split='train')","metadata":{"executionInfo":{"elapsed":3197,"status":"ok","timestamp":1639927291490,"user":{"displayName":"lisa richter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08999729000846221512"},"user_tz":-60},"id":"QB1fi7RhskkP","outputId":"2208c83a-6883-4e4d-9954-ca42283943ba","execution":{"iopub.status.busy":"2022-08-10T14:10:39.127863Z","iopub.execute_input":"2022-08-10T14:10:39.129755Z","iopub.status.idle":"2022-08-10T14:10:43.165118Z","shell.execute_reply.started":"2022-08-10T14:10:39.129703Z","shell.execute_reply":"2022-08-10T14:10:43.163762Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.45k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d204761c16e84a6293576d18ee6e3b11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/881 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1b04ecb25464d92aaa9a21169bbd268"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset tweets_hate_speech_detection/default (download: 2.96 MiB, generated: 3.04 MiB, post-processed: Unknown size, total: 6.00 MiB) to /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.28M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79e8a9cf42cc4df7b477d4b4bc626781"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/31962 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset tweets_hate_speech_detection downloaded and prepared to /root/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"tweet = hate_data_train['tweet']\nlabel = hate_data_train['label']\ndf_hate_1 = pd.DataFrame({'text': tweet, 'label': label}) # 0 = no hate; 1 = hate (racist or sexist)\n","metadata":{"executionInfo":{"elapsed":217,"status":"ok","timestamp":1639927295023,"user":{"displayName":"lisa richter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08999729000846221512"},"user_tz":-60},"id":"jq4tmXAEA0kH","execution":{"iopub.status.busy":"2022-08-10T15:46:00.851063Z","iopub.execute_input":"2022-08-10T15:46:00.851481Z","iopub.status.idle":"2022-08-10T15:46:00.923909Z","shell.execute_reply.started":"2022-08-10T15:46:00.851448Z","shell.execute_reply":"2022-08-10T15:46:00.922917Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# load different hate speech dataset:\nhate_data_2= load_dataset('hate_speech_offensive', split='train')","metadata":{"executionInfo":{"elapsed":3531,"status":"ok","timestamp":1639927299993,"user":{"displayName":"lisa richter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08999729000846221512"},"user_tz":-60},"id":"HUPq8o4UHQOI","outputId":"43db87fe-b7bc-40e9-dd70-cf67b538db7a","execution":{"iopub.status.busy":"2022-08-10T14:10:48.144137Z","iopub.execute_input":"2022-08-10T14:10:48.144568Z","iopub.status.idle":"2022-08-10T14:10:52.420091Z","shell.execute_reply.started":"2022-08-10T14:10:48.144534Z","shell.execute_reply":"2022-08-10T14:10:52.418888Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e30b231a9cd47e9a8b3c82f7e095291"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/823 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"430e6863b21a4560b1f22293c9bcee26"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset hate_speech_offensive/default (download: 2.43 MiB, generated: 3.06 MiB, post-processed: Unknown size, total: 5.49 MiB) to /root/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feb60d72e22c4bc3bfa396e44eb38321"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/24783 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset hate_speech_offensive downloaded and prepared to /root/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"text_2 = hate_data_2['tweet']\nlabel_2 = hate_data_2['class']\n\ndf_hate_2 = pd.DataFrame({'text': text_2, 'label': label_2}) # 0 = hate-speech; 1 = offensive-language; 2 = neither","metadata":{"executionInfo":{"elapsed":205,"status":"ok","timestamp":1639927302966,"user":{"displayName":"lisa richter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08999729000846221512"},"user_tz":-60},"id":"lR2rbmo9HmHw","execution":{"iopub.status.busy":"2022-08-10T14:10:54.791915Z","iopub.execute_input":"2022-08-10T14:10:54.792361Z","iopub.status.idle":"2022-08-10T14:10:54.850159Z","shell.execute_reply.started":"2022-08-10T14:10:54.792322Z","shell.execute_reply":"2022-08-10T14:10:54.849257Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# change label to match df_hate_1 and binary classification: 0: no hate; 1: hate\ndf_hate_2.loc[(df_hate_2.label == 0),'label']=1 # change 0 to 1 to collapse hate speech and offensive language in one class\ndf_hate_2.loc[(df_hate_2.label == 2),'label']=0 # then change 2 to 0 to match df_hate_1 classes\n# 0 = no hate; 1: hate ","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1639927304456,"user":{"displayName":"lisa richter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08999729000846221512"},"user_tz":-60},"id":"my9mfzW6A0hk","execution":{"iopub.status.busy":"2022-08-10T14:10:58.270694Z","iopub.execute_input":"2022-08-10T14:10:58.272090Z","iopub.status.idle":"2022-08-10T14:10:58.285402Z","shell.execute_reply.started":"2022-08-10T14:10:58.272034Z","shell.execute_reply":"2022-08-10T14:10:58.284387Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_hate = pd.concat([df_hate_1, df_hate_2])","metadata":{"executionInfo":{"elapsed":200,"status":"ok","timestamp":1639927306139,"user":{"displayName":"lisa richter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08999729000846221512"},"user_tz":-60},"id":"mQCrtmA1A0c7","execution":{"iopub.status.busy":"2022-08-10T14:11:13.377084Z","iopub.execute_input":"2022-08-10T14:11:13.377491Z","iopub.status.idle":"2022-08-10T14:11:13.387996Z","shell.execute_reply.started":"2022-08-10T14:11:13.377458Z","shell.execute_reply":"2022-08-10T14:11:13.386504Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df_hate = shuffle(df_hate)\ndf_hate = df_hate[:24000]\ndf_hate.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:19:12.766979Z","iopub.execute_input":"2022-08-10T16:19:12.767880Z","iopub.status.idle":"2022-08-10T16:19:12.782141Z","shell.execute_reply.started":"2022-08-10T16:19:12.767841Z","shell.execute_reply":"2022-08-10T16:19:12.781040Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"                                                    text  label\n7439    used the write stuff best piece of writing al...      0\n25259  in pueorico environmental injustice and inflam...      1\n19175  RT Kim K got bitches thinkin being a hoe is a ...      1\n25552  no words . . . self selfie porait selfporait f...      0\n11315  I'm late as hell....Niggas really hating cuz t...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7439</th>\n      <td>used the write stuff best piece of writing al...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25259</th>\n      <td>in pueorico environmental injustice and inflam...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19175</th>\n      <td>RT Kim K got bitches thinkin being a hoe is a ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25552</th>\n      <td>no words . . . self selfie porait selfporait f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11315</th>\n      <td>I'm late as hell....Niggas really hating cuz t...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def dataset_preprocessing(df_hate):\n    df_hate['text'] = df_hate['text'].apply(prepare_text)\n    print(f'non-hate tweets: {len(df_hate[df_hate[\"label\"] == 0])}')\n    print(f'hate tweets: {len(df_hate[df_hate[\"label\"] == 1])}')\n    return df_hate","metadata":{"executionInfo":{"elapsed":219,"status":"ok","timestamp":1639927307466,"user":{"displayName":"lisa richter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08999729000846221512"},"user_tz":-60},"id":"MG3KEwTJJBtP","outputId":"2f1ec0df-4797-4c60-9089-e1e43db95d94","execution":{"iopub.status.busy":"2022-08-10T15:46:09.019326Z","iopub.execute_input":"2022-08-10T15:46:09.019762Z","iopub.status.idle":"2022-08-10T15:46:09.025833Z","shell.execute_reply.started":"2022-08-10T15:46:09.019727Z","shell.execute_reply":"2022-08-10T15:46:09.024614Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# 2. Load XLNet, Prepare Inputs and Define Hyperparameter","metadata":{"id":"2ZpeyyofRkJp"}},{"cell_type":"code","source":"# custom dataset class\nclass HateDataset(Dataset):\n    def __init__(self, reviews, targets, tokenizer, max_len):\n        self.reviews = reviews\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.reviews)\n    \n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        target = self.targets[item]\n        \n        encoding = self.tokenizer.encode_plus(review,\n                                              add_special_tokens=True,\n                                              max_length=self.max_len,\n                                              truncation=True,\n                                              return_token_type_ids=False,\n                                              pad_to_max_length=False,\n                                              return_attention_mask=True,\n                                              return_tensors='pt',)\n        \n        input_ids = pad_sequences(encoding['input_ids'], \n                                  maxlen=MAX_LEN, \n                                  dtype=torch.Tensor ,\n                                  truncating=\"post\",\n                                  padding=\"post\")\n        input_ids = input_ids.astype(dtype = 'int64')\n        input_ids = torch.tensor(input_ids) \n        \n        attention_mask = pad_sequences(encoding['attention_mask'], \n                                       maxlen=MAX_LEN, \n                                       dtype=torch.Tensor ,\n                                       truncating=\"post\",\n                                       padding=\"post\")\n        attention_mask = attention_mask.astype(dtype = 'int64')\n        attention_mask = torch.tensor(attention_mask)       \n        \n        return {'review_text': review,\n                'input_ids': input_ids,\n                'attention_mask': attention_mask.flatten(),\n                'targets': torch.tensor(target, dtype=torch.long)}","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1639927385578,"user":{"displayName":"lisa richter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08999729000846221512"},"user_tz":-60},"id":"yCtFjHiWT_XX","execution":{"iopub.status.busy":"2022-08-10T15:47:33.828212Z","iopub.execute_input":"2022-08-10T15:47:33.828692Z","iopub.status.idle":"2022-08-10T15:47:33.842756Z","shell.execute_reply.started":"2022-08-10T15:47:33.828651Z","shell.execute_reply":"2022-08-10T15:47:33.841504Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# define custom dataloader:\ndef create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = HateDataset(reviews=df.text.to_numpy(),\n                     targets=df.label.to_numpy(),\n                     tokenizer=tokenizer,\n                     max_len=max_len)\n    \n    return DataLoader(ds,\n                    batch_size=batch_size,\n                    num_workers=2)                    ","metadata":{"executionInfo":{"elapsed":215,"status":"ok","timestamp":1639927394727,"user":{"displayName":"lisa richter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08999729000846221512"},"user_tz":-60},"id":"hc7pFLihU3O7","execution":{"iopub.status.busy":"2022-08-10T15:47:36.840424Z","iopub.execute_input":"2022-08-10T15:47:36.841014Z","iopub.status.idle":"2022-08-10T15:47:36.849531Z","shell.execute_reply.started":"2022-08-10T15:47:36.840962Z","shell.execute_reply":"2022-08-10T15:47:36.848083Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# define training step function:\nfrom sklearn import metrics\n\ndef train_epoch(model, data_loader, optimizer, device, scheduler, n_examples, progress_bar):\n    model = model.train()\n    losses = []\n    acc = 0\n    counter = 0\n    \n    for d in data_loader:\n        input_ids = d[\"input_ids\"].reshape(8,512).to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n    \n        outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\n        loss = outputs[0]\n        logits = outputs[1]\n    \n        _, prediction = torch.max(outputs[1], dim=1)\n        targets = targets.cpu().detach().numpy()\n        prediction = prediction.cpu().detach().numpy()\n        accuracy = metrics.accuracy_score(targets, prediction)\n\n        acc += accuracy\n        losses.append(loss.item())\n\n        loss.backward()\n\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n        counter = counter + 1\n    \n    return acc / counter, np.mean(losses)","metadata":{"executionInfo":{"elapsed":204,"status":"ok","timestamp":1639927439654,"user":{"displayName":"lisa richter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08999729000846221512"},"user_tz":-60},"id":"BTZJVJzbWH1t","execution":{"iopub.status.busy":"2022-08-10T15:47:42.874258Z","iopub.execute_input":"2022-08-10T15:47:42.874723Z","iopub.status.idle":"2022-08-10T15:47:42.887188Z","shell.execute_reply.started":"2022-08-10T15:47:42.874689Z","shell.execute_reply":"2022-08-10T15:47:42.885801Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# define evaluation function:\ndef eval_model(model, data_loader, device, n_examples):\n    model = model.eval()\n    losses = []\n    acc = 0\n    counter = 0\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].reshape(8,512).to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            \n            outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels = targets)\n            loss = outputs[0]\n            logits = outputs[1]\n            \n            _, prediction = torch.max(outputs[1], dim=1)\n            targets = targets.cpu().detach().numpy()\n            prediction = prediction.cpu().detach().numpy()\n            accuracy = metrics.accuracy_score(targets, prediction)\n            \n            acc += accuracy\n            losses.append(loss.item())\n            counter += 1\n            \n    return acc / counter, np.mean(losses)","metadata":{"executionInfo":{"elapsed":218,"status":"ok","timestamp":1639927441330,"user":{"displayName":"lisa richter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08999729000846221512"},"user_tz":-60},"id":"uo_4GfYUWf6y","execution":{"iopub.status.busy":"2022-08-10T15:47:45.725007Z","iopub.execute_input":"2022-08-10T15:47:45.725869Z","iopub.status.idle":"2022-08-10T15:47:45.736439Z","shell.execute_reply.started":"2022-08-10T15:47:45.725802Z","shell.execute_reply":"2022-08-10T15:47:45.735362Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# read model saved in previous step:\n# when connected to GPU:\n# model.load_state_dict(torch.load('xlnet_model_hate.bin'))\n\n# when not connected to GPU (hence no cuda):\n#model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/models/xlnet_model_hate.bin', map_location=torch.device('cpu')))","metadata":{"id":"BhgjuEUpW4X0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define function to get prediction of data\ndef get_predictions(model, data_loader):\n    model = model.eval()\n    \n    review_texts = []\n    predictions = []\n    prediction_probs = []\n    real_values = []\n    \n    with torch.no_grad():\n        for d in data_loader:\n            texts = d[\"review_text\"]\n            input_ids = d[\"input_ids\"].reshape(8,512).to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            \n            outputs = model(input_ids=input_ids,\n                            token_type_ids=None,\n                            attention_mask=attention_mask,#\n                            labels=targets)\n            \n            loss = outputs[0]\n            logits = outputs[1]\n            \n            _, preds = torch.max(outputs[1], dim=1)\n            probs = F.softmax(outputs[1], dim=1)\n            \n            review_texts.extend(texts)\n            predictions.extend(preds)\n            prediction_probs.extend(probs)\n            real_values.extend(targets)\n            \n    predictions = torch.stack(predictions).cpu()\n    prediction_probs = torch.stack(prediction_probs).cpu()\n    real_values = torch.stack(real_values).cpu()\n    \n    return review_texts, predictions, prediction_probs, real_values","metadata":{"id":"gfagcMqXXySQ","execution":{"iopub.status.busy":"2022-08-10T15:47:50.668653Z","iopub.execute_input":"2022-08-10T15:47:50.669104Z","iopub.status.idle":"2022-08-10T15:47:50.679920Z","shell.execute_reply.started":"2022-08-10T15:47:50.669066Z","shell.execute_reply":"2022-08-10T15:47:50.678632Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def show_confusion_matrix(confusion_matrix):\n    hmap = sns.heatmap(confusion_matrix/ confusion_matrix.sum(axis=1)[:, np.newaxis], annot=True, fmt='.2%', cmap=\"Blues\")\n    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n\n    plt.ylabel('True Hate')\n    plt.xlabel('Predicted Hate')\n    plt.savefig('confusion_hate');\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-10T15:47:54.103894Z","iopub.execute_input":"2022-08-10T15:47:54.104327Z","iopub.status.idle":"2022-08-10T15:47:54.112762Z","shell.execute_reply.started":"2022-08-10T15:47:54.104290Z","shell.execute_reply":"2022-08-10T15:47:54.111267Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# load pre-trained XLNet model\nfrom transformers import XLNetForSequenceClassification\n\nxlnet_model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels = 2)\n\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')","metadata":{"execution":{"iopub.status.busy":"2022-08-10T15:52:16.653720Z","iopub.execute_input":"2022-08-10T15:52:16.654212Z","iopub.status.idle":"2022-08-10T15:52:22.466844Z","shell.execute_reply.started":"2022-08-10T15:52:16.654176Z","shell.execute_reply":"2022-08-10T15:52:22.465445Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/779k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c24e0a8c53ae4ec5b21539379e36ef10"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:23:25.563009Z","iopub.execute_input":"2022-08-10T16:23:25.563438Z","iopub.status.idle":"2022-08-10T16:23:25.990225Z","shell.execute_reply.started":"2022-08-10T16:23:25.563401Z","shell.execute_reply":"2022-08-10T16:23:25.988899Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"non-hate tweets: 14288\nhate tweets: 9712\n                                                    text  label\n11174   I'll go to class the last 30 minutes of that hoe      1\n7294                        yeah wtf you aren't a pussy.      1\n9263                   Fuck niggah add it up.. gt gt gt       1\n19411         independenceday lee hak buffet restaurant       0\n21997  it's never too late never too late to sta over...      0\n                                                    text  label\n20301  furniture purchased home demo on friday so car...      0\n11174   I'll go to class the last 30 minutes of that hoe      1\n7294                        yeah wtf you aren't a pussy.      1\n5827    she can just fuckin call the venue about it a...      1\n9263                   Fuck niggah add it up.. gt gt gt       1\n                                                    text  label\n20301  furniture purchased home demo on friday so car...      0\n11174   I'll go to class the last 30 minutes of that hoe      1\n7294                        yeah wtf you aren't a pussy.      1\n5827    she can just fuckin call the venue about it a...      1\n30213  hello happy sunday!! have a nice weekend all l...      0\n                                                    text  label\n20301  furniture purchased home demo on friday so car...      0\n5827    she can just fuckin call the venue about it a...      1\n9263                   Fuck niggah add it up.. gt gt gt       1\n30213  hello happy sunday!! have a nice weekend all l...      0\n19411         independenceday lee hak buffet restaurant       0\n                                                    text  label\n20301  furniture purchased home demo on friday so car...      0\n11174   I'll go to class the last 30 minutes of that hoe      1\n7294                        yeah wtf you aren't a pussy.      1\n5827    she can just fuckin call the venue about it a...      1\n9263                   Fuck niggah add it up.. gt gt gt       1\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:16:02.560354Z","iopub.execute_input":"2022-08-10T16:16:02.560817Z","iopub.status.idle":"2022-08-10T16:16:02.570781Z","shell.execute_reply.started":"2022-08-10T16:16:02.560778Z","shell.execute_reply":"2022-08-10T16:16:02.569507Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"text     some bitches care more about their eyebrows th...\nlabel                                                    0\nName: 271, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"report_lst = []\ndf_hate = dataset_preprocessing(df_hate)\ndf_train, df_test = train_test_split(df_hate, test_size=0.25, random_state=101)\nsplits = KFold(n_splits=5, shuffle=True, random_state=42)  # k-fold k=5 as in the BABE paper\n\n\nfor fold, (train_ids, val_ids) in enumerate(splits.split(np.arange(len(df_train)))):\n    df_train.reset_index(drop = True)\n    df_training = df_train.iloc[train_ids]\n    df_val = df_train.iloc[val_ids]\n    \n    model = copy.deepcopy(xlnet_model)\n    model.to(device)\n    MAX_LEN = 512\n    # create training, validation and test set: \n    \n    #df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=101)\n    # define batch size: \n    BATCH_SIZE = 8\n\n    # create data loader for training, validation and test set: \n    train_data_loader = create_data_loader(df_training, tokenizer, MAX_LEN, BATCH_SIZE)\n    val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n    test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n    \n    # defining hyperparameters:\n    EPOCHS = 2\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n                                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}]\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n    total_steps = len(train_data_loader) * EPOCHS\n\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0,\n                                                num_training_steps=total_steps)\n    history = defaultdict(list)\n    best_accuracy = 0\n    num_training_steps = EPOCHS * len(train_data_loader)\n    progress_bar = tqdm(range(num_training_steps))\n\n    for epoch in range(EPOCHS):\n        print(f'Epoch {epoch + 1}/{EPOCHS}')\n        print('-' * 10)\n\n        # call train function\n        train_acc, train_loss = train_epoch(model,\n                                          train_data_loader,\n                                          optimizer,\n                                          device, \n                                          scheduler, \n                                          len(df_train),\n                                            progress_bar)\n\n        print(f'Train loss {train_loss} Train accuracy {train_acc}')\n\n        # call evaluation function\n        val_acc, val_loss = eval_model(model,\n                                     val_data_loader, \n                                     device, \n                                     len(df_val))\n\n        print(f'Val loss {val_loss} Val accuracy {val_acc}')\n        print()\n\n        history['train_acc'].append(train_acc)\n        history['train_loss'].append(train_loss)\n        history['val_acc'].append(val_acc)\n        history['val_loss'].append(val_loss)\n\n        if val_acc > best_accuracy:\n            # torch.save(model.state_dict(), 'xlnet_model_hate.bin') # note: this scrip was implemented using Kaggle's GPU. The model was saved into the Kaggle repo and downloaded manually\n            best_accuracy = val_acc\n    fig = plt.figure()\n    plt.plot(history['train_acc'], label='train accuracy')\n    plt.plot(history['val_acc'], label='validation accuracy')\n    plt.plot(history['train_loss'], label='train loss')\n    plt.plot(history['val_loss'], label='validation loss')\n    #plt.title('Training History')\n    plt.ylabel('Accuracy and Loss')\n    plt.xlabel('Epoch')\n    plt.legend()\n    plt.ylim([0, 1])\n    # fig.savefig('training_hate.png');\n    plt.show()\n    \n    # EVALUATION\n    # call evaluation function and apply to test set\n    test_acc, test_loss = eval_model(model,\n                                     test_data_loader,\n                                     device,\n                                     len(df_test))\n\n    print('Test Accuracy :', test_acc)\n    print('Test Loss :', test_loss)\n    class_names = ['no hate', 'hate']\n    # call prediction function to get actual predictions of test set \n    y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(model, test_data_loader)\n    \n    print(classification_report(y_test, y_pred, target_names=class_names))\n    report = classification_report(y_test, y_pred, target_names=class_names,output_dict=True)\n    report['accuracy'] = {'precision': None,\n        'recall': None,\n        'f1-score': report['accuracy'],\n        'support': None}\n    report = pd.DataFrame(report).transpose()\n    report_lst.append(report)\n    cm = confusion_matrix(y_test, y_pred)\n    df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n    #df_cm.to_csv('cm_hate.csv')\n\n    show_confusion_matrix(df_cm)\n    ","metadata":{},"execution_count":null,"outputs":[]}]}