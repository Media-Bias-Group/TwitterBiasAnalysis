{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b3f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b533ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no file for outlet: BoingBoing\n",
      "no file for outlet: comicsandsdaily\n",
      "no file for outlet: EveningTimesCC\n",
      "no file for outlet: NewYorkSun\n"
     ]
    }
   ],
   "source": [
    "# read user data:\n",
    "user_data = pd.read_csv('../data/twitter/user_ids.csv')\n",
    "twitter_handles = user_data['username'].tolist()\n",
    "\n",
    "# read all articles and their matched tweet (if available; articles for which no tweet was found is also present in the dataset)\n",
    "all_articles = pd.DataFrame(columns=[])\n",
    "dtype={'text': str, 'author_id': str, 'conversation_id': str, 'id': str, 'entities': str, 'attachments': str, 'referenced_tweets': str, 'withheld': str}\n",
    "parse_dates=['created_at']\n",
    "\n",
    "for handle in twitter_handles:\n",
    "    try:\n",
    "        all_articles = all_articles.append(pd.read_csv(f'../data/twitter/article_tweets/{handle}.csv', dtype=dtype, parse_dates=parse_dates)).reset_index(drop=True)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f'no file for outlet: {handle}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ba5bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove entries for which 'id' is missing: these are articles for which no tweet was found:\n",
    "article_tweets = all_articles.dropna(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd392016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0c13a4b",
   "metadata": {},
   "source": [
    "### Set up API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b68e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all necessary functions:\n",
    "def connect_to_twitter(token):\n",
    "    bearer_token = token\n",
    "    return {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "\n",
    "def make_request(headers, params, url):\n",
    "    url=url\n",
    "    params=params\n",
    "    return requests.request(\"GET\", url, params=params, headers=headers).json()\n",
    "\n",
    "def make_df(response):\n",
    "    return pd.DataFrame(response['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5419e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read credentials:\n",
    "#creds = pd.read_csv(f'../../creds/CredentialsAcademicAPI.csv') # read own credentials\n",
    "\n",
    "# define bearer_token:\n",
    "bearer_token = creds.iloc[0]['bearer_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3be8572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# connect to API\n",
    "headers = connect_to_twitter(bearer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67a128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b06b3a6a",
   "metadata": {},
   "source": [
    "### Collecting quoted retweets to tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d051d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract list of tweet ids & created_at (when tweet was created)\n",
    "tweet_ids = article_tweets['id'].tolist()\n",
    "start_dates = article_tweets['created_at'].tolist()\n",
    "\n",
    "# convert to pydatetime\n",
    "for i in range (0, len(start_dates)):\n",
    "    start_dates[i] = start_dates[i].to_pydatetime()\n",
    "\n",
    "# calculate end date (start + 60 days)\n",
    "end_dates = [] # empty list\n",
    "cut_off = '2021-11-23 00:00:00+00:00' # cutoff date (date can't be in future)\n",
    "for i in range (0, len(start_dates)):\n",
    "    end_dates.append(start_dates[i] + datetime.timedelta(+60)) \n",
    "    if end_dates[i] > datetime.datetime.strptime(cut_off, '%Y-%m-%d %H:%M:%S%z'):\n",
    "        end_dates[i] = datetime.datetime.strptime(cut_off, '%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "# convert both lists to strings:\n",
    "start_dates = [str(i) for i in start_dates]\n",
    "end_dates = [str(i) for i in end_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b9dbd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define url\n",
    "url=\"http://api.twitter.com/2/tweets/search/all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b817919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define params\n",
    "for tweet_id, start, end in zip(tweet_ids, start_dates, end_dates):\n",
    "    params={'query': f'{tweet_id}',\n",
    "            'start_time': datetime.datetime.strptime(start, '%Y-%m-%d %H:%M:%S%z').isoformat(),\n",
    "            'end_time': datetime.datetime.strptime(end, '%Y-%m-%d %H:%M:%S%z').isoformat(),\n",
    "            'tweet.fields': 'in_reply_to_user_id,author_id,created_at,conversation_id', \n",
    "            'expansions': 'referenced_tweets.id,in_reply_to_user_id',\n",
    "            'max_results': 500} # with academic access: 500 results per response\n",
    "    \n",
    "    response=make_request(headers, params, url)\n",
    "    time.sleep(4)\n",
    "    \n",
    "    if response['meta']['result_count'] > 0:\n",
    "        response_df = make_df(response)\n",
    "        \n",
    "        if 'next_token' in  response['meta']:\n",
    "            while 'next_token' in  response['meta']:\n",
    "                params['next_token'] = response['meta']['next_token']\n",
    "                response = make_request(headers, params, url)\n",
    "                time.sleep(4) \n",
    "                \n",
    "                if response['meta']['result_count'] > 0:\n",
    "                    response_df = response_df.append(make_df(response))\n",
    "                    \n",
    "                if 'next_token' not in  response['meta']:\n",
    "                    break\n",
    "    \n",
    "        response_df.to_csv(f'../data/twitter/retweet_collection/{tweet_id}_retweet.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
